# -*- coding: utf-8 -*-
"""MV_miniProject_Group-13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1spfbbXtxRlQlk7GeT1zKwFV5YXhbT705
"""

import tensorflow as tf #pip install tensorflow

# Training dataset = 60000   and Testing Samples = 10000
# Tensorflow already contain MNIST data ser which can be loaded using keras

mnist = tf.keras.datasets.mnist  #size = 28*28  images of 0 to 9

# After Loading the MNIST data, Divide into train and test datasets

(x_train, y_train),(x_test, y_test) = mnist.load_data()

x_train.shape

# check the graph and data
import matplotlib.pyplot as plt
plt.imshow(x_train[0])
plt.show()

#to plot it change the configuration because we dont know whether it's color or binary images
plt.imshow(x_train[0], cmap = plt.cm.binary)

# checking the value of each pixel before normalization

print(x_train[0])

# As images are in gray level (1 channer ==> 0 to 255), not colored (RGB)
# Normalizing the Data | Pre-Processing Step

x_train = tf.keras.utils.normalize (x_train, axis = 1)
x_test = tf.keras.utils.normalize(x_test, axis = 1)
plt.imshow(x_train[0], cmap = plt.cm.binary)

print(x_train[0])

print(y_train[0])

# Resizing image to make it suitable for apply Convolutional operation
import numpy as np
image_size=28
# increasing one dimension for kernel operation
x_trainr = np.array(x_train).reshape(-1, image_size, image_size, 1)
x_testr = np.array(x_test).reshape(-1, image_size, image_size, 1)
print("Training Sample Dimensin : ",x_trainr.shape)
print("Testing Sample Dimension : ",x_testr.shape)

# creating a Deep Neural Network
#Training on 60,000 samples of MNIST handwritten dataset

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D

# Now creating a Neural Network
model = Sequential()

#first Convolutional layer 0,1,2,.. (60,000, 28, 28, 1)
# here number of neurons are 64
model.add(Conv2D(64, (3,3), input_shape = x_trainr.shape[1:])) # for first con. layer mention input layer size
model.add(Activation("relu")) #Activation fuction to make it non-linear,<0==remove,>0==allow
model.add(MaxPooling2D(pool_size=(2,2))) #maxpooling single maximum value of 2*2 and reduces the size upto half

# Second Conv. layer
model.add(Conv2D(64, (3,3)))
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(2,2)))

# Third Conv. Layer
model.add(Conv2D(64, (3,3)))
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(2,2)))

# Fully Connected Layer #1
model.add(Flatten()) #convert 2D to 1D
model.add(Dense(64))
model.add(Activation("relu"))

# fully Connected Layer #2
model.add(Dense(32))
model.add(Activation("relu"))

# Fully Connected Layer #Final(last)
#output must be equal to number of classes, 10(0-9)
model.add(Dense(10)) #this last layer must be equal to 10
model.add(Activation('softmax')) # activation fun is chaged to softmax (Class Probabilities)

from sklearn.metrics import f1_score

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))

# Make predictions
y_pred = model.predict_classes(x_test)

# Calculate F1 score
f1 = f1_score(y_test, y_pred, average='macro')
print("F1 score:", f1)

model.summary()

print ("Total Training Samples = ",len(x_trainr))

#compile the Model
model.compile(loss ="sparse_categorical_crossentropy", optimizer ="adam", metrics=['accuracy'])

# treaining the Model
history  = model.fit(x_trainr,y_train,epochs=5,validation_split = 0.3)

# Evaluateing on testing data set MNIST

test_loss, test_acc = model.evaluate(x_testr, y_test)
print ("Test loss on 10,000 test Samples: \n",test_loss)
print ("Validation Accuracy on 10,000 test samples: ",test_acc)

fig, ax = plt.subplots(2,1)
ax[0].plot(history.history['loss'], color='b', label="Training loss")
ax[0].plot(history.history['val_loss'], color='r', label="validation loss",axes =ax[0])
legend = ax[0].legend(loc='best', shadow=True)

ax[1].plot(history.history['accuracy'], color='b', label="Training accuracy")
ax[1].plot(history.history['val_accuracy'], color='r',label="Validation accuracy")
legend = ax[1].legend(loc='best', shadow=True)

# predictions = new_model.predict([x_test])

predictions = model.predict([x_testr])

print(predictions)

#to understand, convert the prediction into one hot coding we are using numpy for that
# argmax returns the maximum value of index and find value of it
print(np.argmax(predictions[1]))

# check that our answer is true or not
plt.imshow(x_test[1])

import cv2
img = cv2.imread("seven.jpg")
plt.imshow(img)

img.shape #because it's a color image

gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

gray.shape

resized = cv2.resize(gray, (28,28), interpolation = cv2.INTER_AREA)
resized.shape

newimg = tf.keras.utils.normalize (resized, axis=1) # 0 to 1 scaling

newimg = np.array(newimg).reshape(-1, image_size , image_size,1) # kernel operation of convolutional layer

newimg.shape

prediction = model.predict(newimg)

print(np.argmax(prediction))